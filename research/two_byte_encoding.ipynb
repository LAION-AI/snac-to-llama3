{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"Class to map codes from huggingface dataset to tokens in Llama 3-8B token\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stoi = {}\n",
    "        self.itos = {}\n",
    "    \n",
    "    def build_vocabulary(self, parquet_files, tokenizer_file=\"tokenizer.json\"):\n",
    "        '''\n",
    "        creates the vocabulary from the Llama 3 tokenizer and hugging face dataset\n",
    "        Args:\n",
    "            tokenizer_file(str): file downloaded from Llama 3(8B) which contains the vocabulary for the model\n",
    "            parquet_files(list): director with the dataset from hugging face in parquet format\n",
    "\n",
    "        '''\n",
    "        # Open the JSON file\n",
    "        with open(tokenizer_file, 'r') as file:\n",
    "            # Load the JSON data\n",
    "            data = json.load(file)\n",
    "        \n",
    "        llama_stoi = data['model']['vocab']\n",
    "        llama_itos = {value:key for key,value in llama_stoi.items()}\n",
    "\n",
    "        #load hugging face data\n",
    "        dataset = load_dataset('parquet', data_files=parquet_files)\n",
    "        vocabulary = set()\n",
    "\n",
    "        for sent in dataset[\"train\"][\"txt\"]:\n",
    "            for word in sent.split():\n",
    "                vocabulary.add(word)\n",
    "        \n",
    "        self.itos = {int(value):llama_itos[int(value)] for value in vocabulary}\n",
    "        self.stoi = {value:key for key,value in self.itos.items()}\n",
    "    \n",
    "    def save(self, file_path):\n",
    "        with open(file_path, \"w\") as file:\n",
    "            json.dump(self.itos, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = [f\"dataset/default/partial-train/000{i}.parquet\" for i in range(10)]\n",
    "dataset = load_dataset('parquet', data_files=train_dir)\n",
    "txt = dataset[\"train\"][\"txt\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tokenization</w>': 2,\n",
       " 'is</w>': 2,\n",
       " 'the</w>': 3,\n",
       " 'process</w>': 1,\n",
       " 'of</w>': 2,\n",
       " 'breaking</w>': 1,\n",
       " 'down</w>': 1,\n",
       " 'a</w>': 1,\n",
       " 'sequence</w>': 1,\n",
       " 'text</w>': 2,\n",
       " 'into</w>': 2,\n",
       " 'smaller</w>': 1,\n",
       " 'units</w>': 1,\n",
       " 'called</w>': 1,\n",
       " 'tokens,</w>': 1,\n",
       " 'which</w>': 1,\n",
       " 'can</w>': 1,\n",
       " 'be</w>': 1,\n",
       " 'w or d s,</w>': 1,\n",
       " 'p h r as es ,</w>': 1,\n",
       " 'or</w>': 1,\n",
       " 'e v en</w>': 1,\n",
       " 'in d i v i d u al</w>': 1,\n",
       " 'ch ar a c te r s</w>': 1,\n",
       " 'of t en</w>': 1,\n",
       " 'f i r s t</w>': 1,\n",
       " 'step </w>': 1,\n",
       " 'in </w>': 1,\n",
       " 'na t ur al</w>': 1,\n",
       " 'l an g u a g e s</w>': 1,\n",
       " 'processing</w>': 2,\n",
       " 't as k s</w>': 1,\n",
       " 'such</w>': 2,\n",
       " 'as</w>': 3,\n",
       " 'c l as s i f ic ation ,</w>': 1,\n",
       " 'na m ed</w>': 1,\n",
       " 'enti ty </w>': 1,\n",
       " 're c o g ni tion ,</w>': 1,\n",
       " 'an d</w>': 1,\n",
       " 's enti m en t</w>': 1,\n",
       " 'an al y s is</w>': 1,\n",
       " 'T h e</w>': 1,\n",
       " 'r es u l t ing</w>': 1,\n",
       " 'tokens</w>': 2,\n",
       " 'are</w>': 2,\n",
       " 'ty p ic all y </w>': 1,\n",
       " 'us ed</w>': 1,\n",
       " 'in p u t</w>': 1,\n",
       " 'to</w>': 2,\n",
       " 'f ur th er</w>': 1,\n",
       " 'step s,</w>': 1,\n",
       " 'v e c to r ization ,</w>': 1,\n",
       " 'wh er e</w>': 1,\n",
       " 'c on v er te d</w>': 1,\n",
       " 'n u m er ic al</w>': 1,\n",
       " 're pr es en t ation s</w>': 1,\n",
       " 'f or</w>': 1,\n",
       " 'm a ch in e</w>': 1,\n",
       " 'l e ar n ing</w>': 1,\n",
       " 'm o d e l s</w>': 1,\n",
       " 'us e</w>': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.geeksforgeeks.org/byte-pair-encoding-bpe-in-nlp/\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"\n",
    "    Given a vocabulary (dictionary mapping words to frequency counts), returns a \n",
    "    dictionary of tuples representing the frequency count of pairs of characters \n",
    "    in the vocabulary.\n",
    "    \"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    \"\"\"\n",
    "    Given a pair of characters and a vocabulary, returns a new vocabulary with the \n",
    "    pair of characters merged together wherever they appear.\n",
    "    \"\"\"\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "def get_vocab(data):\n",
    "    \"\"\"\n",
    "    Given a list of strings, returns a dictionary of words mapping to their frequency \n",
    "    count in the data.\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(int) # word volcabulary\n",
    "    for line in data:\n",
    "        for word in line.split():\n",
    "            vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "    return vocab\n",
    "\n",
    "def byte_pair_encoding(data, n):\n",
    "    \"\"\"\n",
    "    Given a list of strings and an integer n, returns a list of n merged pairs\n",
    "    of characters found in the vocabulary of the input data.\n",
    "    \"\"\"\n",
    "    vocab = get_vocab(data)\n",
    "    for i in range(n):\n",
    "        pairs = get_stats(vocab)\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "    return vocab\n",
    "\n",
    "# Example usage:\n",
    "corpus = '''Tokenization is the process of breaking down \n",
    "a sequence of text into smaller units called tokens,\n",
    "which can be words, phrases, or even individual characters.\n",
    "Tokenization is often the first step in natural languages processing tasks \n",
    "such as text classification, named entity recognition, and sentiment analysis.\n",
    "The resulting tokens are typically used as input to further processing steps,\n",
    "such as vectorization, where the tokens are converted\n",
    "into numerical representations for machine learning models to use.'''\n",
    "data = corpus.split('.')\n",
    "\n",
    "n = 100\n",
    "bpe_pairs = byte_pair_encoding(data, n)\n",
    "bpe_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
