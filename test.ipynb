{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = './tokenizer.json'\n",
    "\n",
    "# Open the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "    # Load the JSON data\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the loaded data (optional)\n",
    "print(len(data['model']['vocab']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngkuissi/miniforge3/envs/laion/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['__key__', '__url__', 'flac', 'json', 'txt'],\n",
      "        num_rows: 18900\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from a local Parquet file\n",
    "\n",
    "test_dir = [f\"dataset/default/partial-train/000{i}.parquet\" for i in range(10)]\n",
    "\n",
    "dataset = load_dataset('parquet', data_files=test_dir)\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = dataset[\"train\"][\"txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "\n",
    "for sent in dataset[\"train\"][\"txt\"]:\n",
    "    for word in sent.split():\n",
    "        vocabulary.add(word)\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = data['model'][\"vocab\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = {value:key for key,value in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos_snac = {int(value):itos[int(value)] for value in vocabulary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"Class to map codes from huggingface dataset to tokens in Llama 3-8B token\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stoi = {}\n",
    "        self.itos = {}\n",
    "    \n",
    "    def build_vocabulary(self, parquet_files, tokenizer_file=\"tokenizer.json\"):\n",
    "        '''\n",
    "        creates the vocabulary from the Llama 3 tokenizer and hugging face dataset\n",
    "        Args:\n",
    "            tokenizer_file(str): file downloaded from Llama 3(8B) which contains the vocabulary for the model\n",
    "            parquet_files(list): director with the dataset from hugging face in parquet format\n",
    "\n",
    "        '''\n",
    "        # Open the JSON file\n",
    "        with open(tokenizer_file, 'r') as file:\n",
    "            # Load the JSON data\n",
    "            data = json.load(file)\n",
    "        \n",
    "        llama_stoi = data['model']['vocab']\n",
    "        llama_itos = {value:key for key,value in llama_stoi.items()}\n",
    "\n",
    "        #load hugging face data\n",
    "        dataset = load_dataset('parquet', data_files=test_dir)\n",
    "        vocabulary = set()\n",
    "\n",
    "        for sent in dataset[\"train\"][\"txt\"]:\n",
    "            for word in sent.split():\n",
    "                vocabulary.add(word)\n",
    "        \n",
    "        self.itos = {int(value):llama_itos[int(value)] for value in vocabulary}\n",
    "        self.stoi = {value:key for key,value in self.itos.items()}\n",
    "    \n",
    "    def save(self, file_path):\n",
    "        with open(file_path, \"w\") as file:\n",
    "            json.dump(self.itos, file)\n",
    "        \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
